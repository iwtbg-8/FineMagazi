import{tokenize}from"../tokenizer/index.js";import{createParser}from"../parser/create.js";import{createGenerator}from"../generator/create.js";import{createConvertor}from"../convertor/create.js";import{createWalker}from"../walker/create.js";import{Lexer}from"../lexer/Lexer.js";import mix from"./config/mix.js";function createSyntax(e){const r=createParser(e),t=createWalker(e),o=createGenerator(e),{fromPlainObject:a,toPlainObject:n}=createConvertor(t),i={lexer:null,createLexer:e=>new Lexer(e,i,i.lexer.structure),tokenize:tokenize,parse:r,generate:o,walk:t,find:t.find,findLast:t.findLast,findAll:t.findAll,fromPlainObject:a,toPlainObject:n,fork(r){const t=mix({},e);return createSyntax("function"==typeof r?r(t,Object.assign):mix(t,r))}};return i.lexer=new Lexer({generic:!0,types:e.types,atrules:e.atrules,properties:e.properties,node:e.node},i),i}export default e=>createSyntax(mix({},e));